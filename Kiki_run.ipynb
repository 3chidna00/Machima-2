{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6303b80d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6850ae2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/project/ai901503-ai0003/kiki/test_edit.csv\")\n",
    "\n",
    "submission = pd.read_csv(\"/project/ai901503-ai0003/kiki/submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5522b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fff272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = '/project/ai901503-ai0003/Model/Qwen3-30B-A3B'\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa1351",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_inputs(question):\n",
    "    # prepare the model input\n",
    "    prompt = '''###Role:\n",
    "                You are an expert in ethical finance.\n",
    "\n",
    "                ###Objective:\n",
    "                Carefully read the question and select the **single best answer** from the following choices:\n",
    "\n",
    "                A, B, C, D, E, Rise, Fall\n",
    "\n",
    "                ###Instructions:\n",
    "                - Only use one of the provided options: **A, B, C, D, E, Rise, Fall**\n",
    "                - Your final answer must be wrapped **exactly** in: <answer>YourAnswer</answer>\n",
    "                - Do **not** include any extra text, explanation, or formatting unless specified.\n",
    "\n",
    "\n",
    "            '''\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": question+\"let's think step by step and answer the question.\"},\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking= False # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    return tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "def model_generate(question):   \n",
    "    # conduct text completion\n",
    "    generated_ids = model.generate(\n",
    "        **get_model_inputs(question),\n",
    "        max_new_tokens=10000,\n",
    "        use_cache = True\n",
    "    )\n",
    "    output_ids = generated_ids[0][len(get_model_inputs(question).input_ids[0]):].tolist()\n",
    "\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)\n",
    "    print(\"token length:\", len(output_ids))\n",
    "    return thinking_content, content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c83f3d",
   "metadata": {},
   "source": [
    "# Model VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e94b15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb557d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/project/ai901503-ai0003/kiki/test_edit.csv\")\n",
    "submission = pd.read_csv(\"/project/ai901503-ai0003/kiki/submission.csv\")\n",
    "model_name = '/project/ai901503-ai0003/Model/Qwen3-30B-A3B'\n",
    "tokenizer_name = '/project/ai901503-ai0003/kiki/model/typhoon2.1-gemma3-12b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7143633",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ff8ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, top_k=10, min_p=0, max_tokens=4096)\n",
    "model = LLM(model=model_name, dtype=torch.float16, tensor_parallel_size=4, task=\"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114fcbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def model_generate(question, system_prompt):\n",
    "    system_prompt = '''\n",
    "        ###Role\n",
    "        You are a financial time series expert. \n",
    "\n",
    "        ###Instructions\n",
    "        The following is a summary of engineered features and exploratory data analysis from a stock price dataset. \n",
    "        Use this information to reason whether the price is likely to rise or fall in the next 5 days.\n",
    "\n",
    "        ###Details\n",
    "        - Rise is when the price increases after 5 days.\n",
    "        - Fall is when the price decreases after 5 days.\n",
    "\n",
    "        ###Output\n",
    "        1. **Always** give short reasons why the price is likely to rise or fall.\n",
    "        2. Use concise language.\n",
    "        3. Answer with a single choice: \"Rise\" or \"Fall\". **ONLY use these two options**\n",
    "        4. Your final answer must be wrapped **exactly** in: <answer>YourAnswer</answer> **THIS IS VERY IMPORTANT**\n",
    "\n",
    "\n",
    "        '''\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question+'/think'+\"Let's think step by step and answer the question.\"},\n",
    "    ]\n",
    "    \n",
    "    outputs = model.chat(messages, sampling_params)\n",
    "    token = len(tokenizer.encode(outputs[0].outputs[0].text, add_special_tokens=False))\n",
    "    print(f\"Token length: {token}\")\n",
    "    print(outputs[0].outputs[0].text)\n",
    "    \n",
    "    match = re.search(r'<answer>(.*?)</answer>', outputs[0].outputs[0].text)\n",
    "\n",
    "    if match:\n",
    "        result = match.group(1)\n",
    "        print(result)\n",
    "        return result\n",
    "    else:\n",
    "        print(np.nan)\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9b5ae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stock_df = get_stock_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f7641",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "stock_submission = stock_df.copy()\n",
    "stock_submission['answer'] = np.nan\n",
    "stock_submission = stock_submission.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af26c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for n,question in enumerate(tqdm.tqdm(stock_df['query'])):\n",
    "    prompt = cal_stat(question)\n",
    "    print(prompt)\n",
    "    output = model_generate(prompt, system_prompt)\n",
    " \n",
    "    stock_submission.loc[n,\"ansewr\"] = output\n",
    "    stock_submission.to_csv(\"/project/ai901503-ai0003/kiki/submission_stock_Final.csv\", index=False)\n",
    "    print(\"\\n--------------------------------------------------\\n\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4faf4c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('/project/ai901503-ai0003/kiki/Qwen3-30B-A3-English_Prompt-Thinking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75efb1b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_submission =  submission.merge(stock_submission, on=\"id\", how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42aab33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_submission['answer_y'] = final_submission['answer_y'].fillna(final_submission['answer_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543358aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dcad70",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "super_final_submission = final_submission[['id', 'answer_y']].rename(columns={'answer_y': 'answer'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bd831",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "super_final_submission.to_csv(\"/project/ai901503-ai0003/kiki/submission_Rise_fall_zeroshot_abitCoT.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
